{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "# import transformers\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer # type: ignore\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "harmful_behaviors = \"../adv_bench/harmful_behaviors.csv\"\n",
    "prompt_sentences_path = \"../prompt-sentences-main/prompt_sentences.json\"\n",
    "embedding_path = \"../prompt-sentences-main/prompt_sentences-all-minilm-l6-v2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_entry(text_list, ref_number, label_name):\n",
    "    text_entry_list = []\n",
    "    for text in text_list:\n",
    "        text_entry = {}\n",
    "        text_entry[\"text\"] = text\n",
    "        text_entry[\"ref\"] = ref_number\n",
    "        text_entry[\"embedding\"] = []\n",
    "        text_entry_list.append(text_entry)\n",
    "    dict_entry[\"label\"] = label_name\n",
    "    dict_entry[\"prompts\"] = text_entry_list\n",
    "    dict_entry[\"centroid\"] = []\n",
    "    return dict_entry\n",
    "\n",
    "def save_dict_as_json(dict_entry, out_file_name=\"prompt_sentences.json\"):\n",
    "    with open(out_file_name, \"w\") as outfile: \n",
    "        json.dump(dict_entry, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embedding_path) as f:\n",
    "    embedding_data = json.load(f)\n",
    "    \n",
    "with open(prompt_sentences_path) as f:\n",
    "    sentences_data = json.load(f)\n",
    "    \n",
    "harmful_data = pd.read_csv(harmful_behaviors)\n",
    "\n",
    "goal_list = list(harmful_data['goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_entry = value_entry(goal_list, 11, \"harmful behaviors\")\n",
    "sentences_data[\"negative_values\"].append(dict_entry)\n",
    "save_dict_as_json(sentences_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tiagomachado/Documents/GitHub/MyProjects/Responsible_Prompt/responsible-prompting-api/clustering/../customize/customize_embeddings.py\", line 46, in <module>\n",
      "    prompt_json = json.load(open(json_in_file))\n",
      "                            ^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'prompt-sentences-main/prompt_sentences.json'\n"
     ]
    }
   ],
   "source": [
    "!python ../customize/customize_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3==1.26.7\n",
      "  Obtaining dependency information for urllib3==1.26.7 from https://files.pythonhosted.org/packages/af/f4/524415c0744552cce7d8bf3669af78e8a069514405ea4fcbd0cc44733744/urllib3-1.26.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m163.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m507.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "Successfully installed urllib3-1.26.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade 'urllib3==1.26.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMT-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
