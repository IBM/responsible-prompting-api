{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27dbd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d10a2a",
   "metadata": {},
   "source": [
    "### Loading HuggingFace Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5185c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    COLAB = True\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "else:\n",
    "    COLAB = False\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a900a",
   "metadata": {},
   "source": [
    "### Embedding Model IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ed4cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"intfloat/multilingual-e5-large\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2379ed",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ae38e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts model_id into filenames\n",
    "def model_id_to_filename( model_id ):\n",
    "    return model_id.split('/')[1].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7f605",
   "metadata": {},
   "source": [
    "### Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fe3a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings( texts, model_id ):\n",
    "    # Warning in case of prompts longer than 256 words\n",
    "    for t in texts :\n",
    "        n_words = len( re.split(r\"\\s+\", t ) )\n",
    "        if( n_words > 256 and model_id == \"sentence-transformers/all-MiniLM-L6-v2\" ):\n",
    "            warnings.warn( \"Warning: Sentence provided is longer than 256 words. Model all-MiniLM-L6-v2 expects sentences up to 256 words.\" )\n",
    "            warnings.warn( \"Word count: {}\".format( n_words ) )\n",
    "\n",
    "    if( model_id == 'sentence-transformers/all-MiniLM-L6-v2' ):\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        out = model.encode( texts ).tolist()\n",
    "    else:\n",
    "        # api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "        api_url = f\"https://router.huggingface.co/hf-inference/models/{model_id}/pipeline/feature-extraction\"\n",
    "        headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "        print( \"Request url: \" + api_url )\n",
    "        response = requests.post(api_url, headers=headers, json={\"inputs\": texts })\n",
    "        out = response.json()\n",
    "        \n",
    "    if( 'error' in out ):\n",
    "        return out\n",
    "    while( len( out ) < 384 ):\n",
    "        out = out[0]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a99c88",
   "metadata": {},
   "source": [
    "### Calculate Centroid Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57dc0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid( cluster, dimension = 384, k = 10 ):\n",
    "    centroid = [0] * dimension\n",
    "    count = 0\n",
    "    for prompt in cluster['prompts']:\n",
    "        i = 0\n",
    "        while i < len( prompt['embedding'] ):\n",
    "            centroid[i] += prompt['embedding'][i]\n",
    "            i += 1\n",
    "        count += 1\n",
    "        \n",
    "    i = 0\n",
    "    while i < len( centroid ):\n",
    "        centroid[i] /= count\n",
    "        i += 1\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3227e",
   "metadata": {},
   "source": [
    "### Populating JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "affc3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening existing file locally:  ../prompt-sentences-main/prompt_sentences.json\n",
      "Opening existing file locally:  ../prompt-sentences-main/prompt_sentences-all-minilm-l6-v2.json\n",
      "Opening existing file locally:  ../prompt-sentences-main/prompt_sentences-all-minilm-l6-v2.json\n",
      "Dimensions from hugging face API response: 384\n",
      "Dimensions from json file: 384\n",
      "Dimensions from hugging face API response: 384\n",
      "Dimensions from json file: 384\n",
      "Opening existing file locally:  ../prompt-sentences-main/prompt_sentences-bge-large-en-v1.5.json\n",
      "Request url: https://router.huggingface.co/hf-inference/models/BAAI/bge-large-en-v1.5/pipeline/feature-extraction\n",
      "Opening existing file locally:  ../prompt-sentences-main/prompt_sentences-bge-large-en-v1.5.json\n",
      "Request url: https://router.huggingface.co/hf-inference/models/BAAI/bge-large-en-v1.5/pipeline/feature-extraction\n",
      "Dimensions from hugging face API response: 1024\n",
      "Dimensions from json file: 1024\n",
      "Dimensions from hugging face API response: 1024\n",
      "Dimensions from json file: 1024\n",
      "Opening existing file locally:  ../prompt-sentences-main/prompt_sentences-multilingual-e5-large.json\n",
      "Request url: https://router.huggingface.co/hf-inference/models/intfloat/multilingual-e5-large/pipeline/feature-extraction\n",
      "Opening existing file locally:  ../prompt-sentences-main/prompt_sentences-multilingual-e5-large.json\n",
      "Request url: https://router.huggingface.co/hf-inference/models/intfloat/multilingual-e5-large/pipeline/feature-extraction\n",
      "Dimensions from hugging face API response: 1024\n",
      "Dimensions from json file: 1024\n",
      "Old prompts:  2217\n",
      "New prompts:  0\n",
      "Errors:  0\n",
      "Successes:  0\n",
      "Updating centroids.\n",
      "Dimensions from hugging face API response: 1024\n",
      "Dimensions from json file: 1024\n",
      "Old prompts:  2217\n",
      "New prompts:  0\n",
      "Errors:  0\n",
      "Successes:  0\n",
      "Updating centroids.\n",
      "Saving into file:  ../prompt-sentences-main/prompt_sentences-multilingual-e5-large.json\n",
      "Saving into file:  ../prompt-sentences-main/prompt_sentences-multilingual-e5-large.json\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if( COLAB ):\n",
    "    json_folder = 'https://raw.githubusercontent.com/IBM/responsible-prompting-api/refs/heads/main/prompt-sentences-main/'\n",
    "else:\n",
    "    json_folder = '../prompt-sentences-main/'\n",
    "\n",
    "json_in_file = json_folder + 'prompt_sentences.json'\n",
    "\n",
    "# Trying to open the files first\n",
    "if( COLAB ):\n",
    "    prompt_json_in = requests.get( json_in_file ).json()\n",
    "    print( 'Opening file from GitHub repo: ', json_in_file )\n",
    "else:\n",
    "    if( os.path.isfile( json_in_file ) ):\n",
    "        prompt_json_in = json.load( open( json_in_file ) )\n",
    "        print( 'Opening existing file locally: ', json_in_file )\n",
    "\n",
    "for model_id in model_ids:\n",
    "\n",
    "    json_out_file_suffix = model_id_to_filename( model_id )\n",
    "    json_out_file = f\"{json_folder}prompt_sentences-{json_out_file_suffix}.json\"\n",
    "\n",
    "    # Trying to open the files first\n",
    "    if( COLAB ):\n",
    "        prompt_json_out = requests.get( json_out_file ).json()\n",
    "        print( 'Opening file from GitHub repo: ', json_out_file )\n",
    "    else:\n",
    "        if( os.path.isfile( json_out_file ) ):\n",
    "            prompt_json_out = json.load( open( json_out_file ) )\n",
    "            print( 'Opening existing file locally: ', json_out_file )\n",
    "        else:\n",
    "            # Creating an empty file for new transformer\n",
    "            print( 'Starting a file from scratch for model: ', model_id )\n",
    "\n",
    "    # API request test\n",
    "    api_response_dimensions = len( calculate_embeddings( ['testing API endpoint'], model_id ) )\n",
    "    print( f\"Dimensions from hugging face API response: {api_response_dimensions}\" )\n",
    "    json_file_dimensions = len( prompt_json_out['positive_values'][0]['prompts'][0]['embedding'] )\n",
    "    print( f\"Dimensions from json file: {json_file_dimensions}\" )\n",
    "    if( api_response_dimensions != json_file_dimensions ):\n",
    "        warnings.warn( f\"Dimensions are different: API={api_response_dimensions} while JSON sentences file={json_file_dimensions}\" )\n",
    "\n",
    "prompts_embeddings = {}\n",
    "new_prompts = 0\n",
    "old_prompts = 0\n",
    "errors = 0\n",
    "successes = 0\n",
    "\n",
    "for cluster in prompt_json_out['positive_values']:\n",
    "    for prompt in cluster['prompts']:\n",
    "        if( prompt['embedding'] != [] ):\n",
    "            prompts_embeddings[ prompt['text'] ] = prompt['embedding']\n",
    "\n",
    "for cluster in prompt_json_out['negative_values']:\n",
    "    for prompt in cluster['prompts']:\n",
    "        if( prompt['embedding'] != [] ):\n",
    "            prompts_embeddings[ prompt['text'] ] = prompt['embedding']\n",
    "\n",
    "# Loading all prompts from prompt_json_in, potentially with new/changed sentences\n",
    "\n",
    "# Iterate over the two positive and negative lists\n",
    "for cluster in prompt_json_in['positive_values']:\n",
    "    for prompt in cluster['prompts']:\n",
    "        if( prompt['text'] in prompts_embeddings ):\n",
    "            # Prompt found, no need to request embeddings\n",
    "            prompt['embedding'] = prompts_embeddings[ prompt['text'] ]\n",
    "            old_prompts += 1\n",
    "        else:\n",
    "            # Requesting embedding for new/changed prompt\n",
    "            embedding = calculate_embeddings( prompt['text'], model_id )\n",
    "            if( 'error' in embedding ):\n",
    "                errors += 1\n",
    "            else:\n",
    "                # Add the new/changed prompt to the hashmap\n",
    "                prompts_embeddings[ prompt['text'] ] = embedding\n",
    "\n",
    "                # Using the new hash\n",
    "                prompt['embedding'] = prompts_embeddings[ prompt['text'] ]\n",
    "                successes += 1\n",
    "            new_prompts += 1\n",
    "\n",
    "for cluster in prompt_json_in['negative_values']:\n",
    "    for prompt in cluster['prompts']:\n",
    "        if( prompt['text'] in prompts_embeddings ):\n",
    "            # Prompt found, no need to request embeddings\n",
    "            prompt['embedding'] = prompts_embeddings[ prompt['text'] ]\n",
    "            old_prompts += 1\n",
    "        else:\n",
    "            # Requesting embedding for new/changed prompt\n",
    "            embedding = calculate_embeddings( prompt['text'], model_id )\n",
    "            if( 'error' in embedding ):\n",
    "                errors += 1\n",
    "            else:\n",
    "                # Add the new/changed prompt to the hashmap\n",
    "                prompts_embeddings[ prompt['text'] ] = embedding\n",
    "\n",
    "                # Using the new hash\n",
    "                prompt['embedding'] = prompts_embeddings[ prompt['text'] ]\n",
    "                successes += 1\n",
    "            new_prompts += 1\n",
    "\n",
    "print( 'Old prompts: ', old_prompts )\n",
    "print( 'New prompts: ', new_prompts )\n",
    "print( 'Errors: ', errors )\n",
    "print( 'Successes: ', successes )\n",
    "\n",
    "# After all the embeddings are populated (with no errors), compute the centroids for each value\n",
    "if( errors == 0 ):\n",
    "    print( 'Updating centroids.' )\n",
    "    for cluster in prompt_json_in['positive_values']:\n",
    "        cluster['centroid'] = get_centroid( cluster, json_file_dimensions, 10 )\n",
    "    for cluster in prompt_json_in['negative_values']:\n",
    "        cluster['centroid'] = get_centroid( cluster, json_file_dimensions, 10 )\n",
    "\n",
    "# Saving the embeddings for a specific LLM\n",
    "if( COLAB ):\n",
    "    json_out_file = f\"prompt_sentences-{json_out_file_suffix}.json\"\n",
    "\n",
    "with open( json_out_file, 'w') as outfile:\n",
    "    print( 'Saving into file: ', json_out_file )\n",
    "    json.dump( prompt_json_in, outfile)\n",
    "    print( '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe447a9",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531911d",
   "metadata": {},
   "source": [
    "### Metric 1 - Inter-Cluster Centroid Distance (higher is better) :\n",
    "Calculates the Euclidean distances between centroid vectors and returns their mean.\n",
    "\n",
    "#### Steps:\n",
    "- accumulate all centroid vectors from both positive and negative clusters\n",
    "- calculate pairwise Euclidean distances between all centroids\n",
    "- return their mean\n",
    "\n",
    "### Metric 2 - Misclassification Rate (lower is better) :\n",
    "Counts the number of sentences which are assigned to wrong clusters based on Euclidean distance.\n",
    "\n",
    "#### Steps:\n",
    "- for each prompt:\n",
    "  - calculate Euclidean distance to its assigned cluster's centroid\n",
    "  - compare with distance to all other centroid vectors\n",
    "  - count as misclassified if closer to another cluster's centroid\n",
    "- return total misclassification rate: $$\\frac{\\text{positive\\_misclassified} + \\text{negative\\_misclassified}}{\\text{total\\_prompts}}$$\n",
    "\n",
    "### Metric 3 - Intra-Cluster K-Means Distance (lower is better)\n",
    "Measures cluster cohesiveness.\n",
    "\n",
    "#### Steps:\n",
    "- for each cluster\n",
    "  - calculate Euclidean distance from each prompt to the centroid vector\n",
    "  - calculate the mean value for all the distances in that cluster\n",
    "- return the mean of the average distances from all clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344baaf7",
   "metadata": {},
   "source": [
    "### Metric 1 - Inter-Cluster Centroid Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b94ad184",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inter-cluster centroid distance\n",
    "# Mean of distances between each cluster to its nearest neighbor\n",
    "# Higher is better\n",
    "\n",
    "def calculate_metric_1(model_id):\n",
    "\n",
    "    json_out_file_suffix = model_id_to_filename(model_id)\n",
    "    json_out_file = os.path.join(\"..\", \"prompt-sentences-main\", f\"prompt_sentences-{json_out_file_suffix}.json\")\n",
    "\n",
    "    with open(json_out_file, 'r') as infile:\n",
    "        populated_json_file = json.load(infile)\n",
    "\n",
    "    ## accumulating centroid vectors from positive and negative clusters\n",
    "    centroid_vectors = []\n",
    "\n",
    "    ## positive clusters\n",
    "    for cluster in populated_json_file['positive_values']:\n",
    "        centroid_vectors.append(cluster['centroid'])\n",
    "\n",
    "    ## negative clusters\n",
    "    for cluster in populated_json_file['negative_values']:\n",
    "        centroid_vectors.append(cluster['centroid'])\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for current_centroid in centroid_vectors:\n",
    "        current_distance = float('inf')\n",
    "        for other_centroid in centroid_vectors:\n",
    "            if current_centroid != other_centroid:\n",
    "                distance = np.linalg.norm(np.array(current_centroid) - np.array(other_centroid))\n",
    "                current_distance = min(current_distance, distance)\n",
    "        distances.append(current_distance)\n",
    "\n",
    "    return np.mean(distances) if distances else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb4ba169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model_id  inter_cluster_centroid_distance\n",
      "0       all-minilm-l6-v2                         0.443066\n",
      "1      bge-large-en-v1.5                         0.350401\n",
      "2  multilingual-e5-large                         0.216814\n"
     ]
    }
   ],
   "source": [
    "metric_1_scores = {}\n",
    "\n",
    "for model_id in model_ids:\n",
    "    score = calculate_metric_1(model_id)\n",
    "    metric_1_scores[model_id_to_filename(model_id)] = score\n",
    "\n",
    "df_metric_1 = pd.DataFrame(list(metric_1_scores.items()), columns=['model_id', 'inter_cluster_centroid_distance'])\n",
    "print(df_metric_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0fbe8d",
   "metadata": {},
   "source": [
    "### Metric 2 - Misclassification Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b42fc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Misclassification Rate\n",
    "# If centroid of another cluster is closer to a sentence embedding than its own cluster centroid, the sentence is misclassified.\n",
    "# Lower is better.\n",
    "\n",
    "def calculate_metric_2(model_id):\n",
    "\n",
    "    json_out_file_suffix = model_id_to_filename(model_id)\n",
    "    json_out_file = os.path.join(\"..\", \"prompt-sentences-main\", f\"prompt_sentences-{json_out_file_suffix}.json\")\n",
    "\n",
    "    with open(json_out_file, 'r') as infile:\n",
    "        populated_json_file = json.load(infile)\n",
    "\n",
    "    ## accumulating centroid vectors from positive and negative clusters\n",
    "    centroid_vectors = []\n",
    "\n",
    "    ## positive clusters\n",
    "    for cluster in populated_json_file['positive_values']:\n",
    "        centroid_vectors.append(cluster['centroid'])\n",
    "\n",
    "    ## negative clusters\n",
    "    for cluster in populated_json_file['negative_values']:\n",
    "        centroid_vectors.append(cluster['centroid'])\n",
    "\n",
    "    positive_misclassified_count = 0\n",
    "    negative_misclassified_count = 0\n",
    "    total_number_of_prompts = 0\n",
    "\n",
    "    ## positive clusters\n",
    "    for cluster in populated_json_file['positive_values']:\n",
    "\n",
    "        assigned_centroid = cluster['centroid']\n",
    "        for prompt in cluster['prompts']:\n",
    "\n",
    "            prompt_embedding = prompt['embedding']\n",
    "\n",
    "            own_distance = np.linalg.norm(np.array(prompt_embedding) - np.array(assigned_centroid))\n",
    "\n",
    "            for other_centroid in centroid_vectors:\n",
    "                if other_centroid != assigned_centroid:\n",
    "                    other_distance = np.linalg.norm(np.array(prompt_embedding) - np.array(other_centroid))\n",
    "                    if other_distance < own_distance:\n",
    "                        positive_misclassified_count += 1\n",
    "                        break\n",
    "\n",
    "            total_number_of_prompts += 1\n",
    "\n",
    "    ## negative clusters\n",
    "    for cluster in populated_json_file['negative_values']:\n",
    "\n",
    "        assigned_centroid = cluster['centroid']\n",
    "        for prompt in cluster['prompts']:\n",
    "\n",
    "            prompt_embedding = prompt['embedding']\n",
    "\n",
    "            own_distance = np.linalg.norm(np.array(prompt_embedding) - np.array(assigned_centroid))\n",
    "\n",
    "            for other_centroid in centroid_vectors:\n",
    "                if other_centroid != assigned_centroid:\n",
    "                    other_distance = np.linalg.norm(np.array(prompt_embedding) - np.array(other_centroid))\n",
    "                    if other_distance < own_distance:\n",
    "                        negative_misclassified_count += 1\n",
    "                        break\n",
    "\n",
    "            total_number_of_prompts += 1\n",
    "\n",
    "    total_misclassified_count = positive_misclassified_count + negative_misclassified_count\n",
    "    misclassified_count_rate = total_misclassified_count / total_number_of_prompts\n",
    "\n",
    "    return misclassified_count_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6ea5045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model_id  misclassification_rate\n",
      "0       all-minilm-l6-v2                0.328823\n",
      "1      bge-large-en-v1.5                0.313036\n",
      "2  multilingual-e5-large                0.335589\n"
     ]
    }
   ],
   "source": [
    "metric_2_scores = {}\n",
    "\n",
    "for model_id in model_ids:\n",
    "    metric_2_scores[model_id_to_filename(model_id)] = calculate_metric_2(model_id)\n",
    "\n",
    "df_metric_2 = pd.DataFrame(list(metric_2_scores.items()), columns=['model_id', 'misclassification_rate'])\n",
    "print(df_metric_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09846912",
   "metadata": {},
   "source": [
    "### Metric 3 - Intra-Cluster K-Means Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c192f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intra-cluster K-Means Distance\n",
    "# Measures the mean cluster cohesiveness.\n",
    "# Lower is better.\n",
    "\n",
    "def calculate_metric_3(model_id):\n",
    "\n",
    "    json_out_file_suffix = model_id_to_filename(model_id)\n",
    "    json_out_file = os.path.join(\"..\", \"prompt-sentences-main\", f\"prompt_sentences-{json_out_file_suffix}.json\")\n",
    "\n",
    "    with open(json_out_file, 'r') as infile:\n",
    "        populated_json_file = json.load(infile)\n",
    "    \n",
    "    ## accumulating centroid vectors from positive and negative clusters\n",
    "    centroid_vectors = []\n",
    "\n",
    "    ## positive clusters\n",
    "    for cluster in populated_json_file['positive_values']:\n",
    "        centroid_vectors.append(cluster['centroid'])\n",
    "\n",
    "    ## negative clusters\n",
    "    for cluster in populated_json_file['negative_values']:\n",
    "        centroid_vectors.append(cluster['centroid'])\n",
    "\n",
    "    k_means_distances = []\n",
    "\n",
    "    ## positive_clusters\n",
    "    for cluster in populated_json_file['positive_values']:\n",
    "        \n",
    "        centroid_vector = cluster['centroid']\n",
    "        distances = []\n",
    "\n",
    "        for prompt in cluster['prompts']:\n",
    "            prompt_embedding = prompt['embedding']\n",
    "            distance = np.linalg.norm(np.array(prompt_embedding) - np.array(centroid_vector))\n",
    "            distances.append(distance)\n",
    "\n",
    "        k_means_distances.append(np.mean(distances))\n",
    "\n",
    "    ## negative_clusters\n",
    "    for cluster in populated_json_file['negative_values']:\n",
    "\n",
    "        centroid_vector = cluster['centroid']\n",
    "        distances = []\n",
    "\n",
    "        for prompt in cluster['prompts']:\n",
    "            prompt_embedding = prompt['embedding']\n",
    "            distance = np.linalg.norm(np.array(prompt_embedding) - np.array(centroid_vector))\n",
    "            distances.append(distance)\n",
    "\n",
    "        k_means_distances.append(np.mean(distances))\n",
    "\n",
    "    return np.mean(k_means_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18717dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model_id  intra_cluster_k_means_distance\n",
      "0       all-minilm-l6-v2                        0.733973\n",
      "1      bge-large-en-v1.5                        0.560765\n",
      "2  multilingual-e5-large                        0.359811\n"
     ]
    }
   ],
   "source": [
    "metric_3 = {}\n",
    "\n",
    "for model_id in model_ids:\n",
    "    metric_3[model_id_to_filename(model_id)] = calculate_metric_3(model_id)\n",
    "\n",
    "df_metric_3 = pd.DataFrame(list(metric_3.items()), columns=['model_id', 'intra_cluster_k_means_distance'])\n",
    "print(df_metric_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11660eb",
   "metadata": {},
   "source": [
    "### Embedding Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9ff7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all metrics\n",
    "\n",
    "def evaluate_embedding_model(model_id):\n",
    "\n",
    "    metric_1 = calculate_metric_1(model_id)\n",
    "\n",
    "    metric_2 = calculate_metric_2(model_id)\n",
    "\n",
    "    metric_3 = calculate_metric_3(model_id)\n",
    "\n",
    "    return {\n",
    "        \"model_id\": model_id_to_filename(model_id),\n",
    "        \"metric_1 (higher is better)\": metric_1,\n",
    "        \"metric_2 (lower is better)\": metric_2,\n",
    "        \"metric_3 (lower is better)\": metric_3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba63841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model_id  metric_1 (higher is better)  metric_2 (lower is better)  metric_3 (lower is better)\n",
      "0       all-minilm-l6-v2                     0.443066                    0.328823                    0.733973\n",
      "1      bge-large-en-v1.5                     0.350401                    0.313036                    0.560765\n",
      "2  multilingual-e5-large                     0.216814                    0.335589                    0.359811\n"
     ]
    }
   ],
   "source": [
    "embedding_model_scores = []\n",
    "\n",
    "for model_id in model_ids:\n",
    "    scores = evaluate_embedding_model(model_id)\n",
    "    embedding_model_scores.append(scores)\n",
    "\n",
    "df_embedding_model_scores = pd.DataFrame(embedding_model_scores)\n",
    "\n",
    "## some pandas settings altered so the table appears well\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "print(df_embedding_model_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
